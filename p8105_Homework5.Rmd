---
title: "P8105 Homework 5"
output: github_document
date: "2024-11-14"
---
Name: Xi Peng   Uni: xp2213

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(dplyr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(924)
```

# Problem 1. Birthday problem
Defines the bday_sim function, which simulates random birthdays for a group of size n and checks if there are any duplicate birthdays,returning TRUE if duplicates exist and FALSE otherwise.
```{r}
bday_sim = function(n) {
  
      bdays = sample(1:365, size = n, replace = TRUE)
      
      duplicate = length(unique(bdays)) < n

      return(duplicate)
      
}
```
Runs the bday_sim function 10,000 times for each group size from 2 to 50, calculates the probability of at least two people sharing a birthday, and plots this probability as a function of group size.
```{r}
sim_res =
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n,bday_sim)) |> 
  group_by(n) |> 
  summarize(probability = mean(res))

sim_res |> 
  ggplot(aes(x = n, y = probability)) +
  geom_line() +
  geom_point() +
  labs (
    title = "Likelihood of Birthday Matches in Groups of Varying Sizes",
    x = "Group Size (n)",
    y = "Probability"
  )
```

The plot shows that the probability of at least two people sharing a birthday increases steadily as the group size grows. At around the sample size of 23 people, the probability crosses the 0.5 mark, representing a key threshold. Beyond this point, the probability approaches 1 as the group size exceeds 50.


# Problem 2. Power Analysis of a One-Sample T-test

```{r}
sim_mean_sd = function(n, mu = 0, sigma = 5) {
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  t_test_res = t.test(sim_data$x, mu = 0) |> 
    broom::tidy() |> 
    select(estimate, p.value)
  
  return(t_test_res)
}

sim_res = 
  expand_grid(
    sample_size = 30,
    mu = c(0, 1, 2, 3, 4, 5, 6),
    iter = 1:5000
  ) |> 
  mutate(
    test_res = map(mu, \(x) sim_mean_sd(n = 30, mu = x))
  ) |>
  unnest(test_res) |>
  mutate(
    rejected = p.value < 0.05
  )

sim_q2_summary_df = sim_res |> 
  group_by(mu) |>
  summarise(
    power = mean(rejected),
    avg_estimate = mean(estimate),
    avg_estimate_rejected = mean(estimate[rejected])
  ) 

knitr::kable(sim_q2_summary_df)
```

```{r}
power_plot = 
  sim_q2_summary_df  |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True value of μ",
    y = "Power of test",
    title = "Effect Size and Power"
  ) +
  theme_minimal()

power_plot
```

```{r}
estimates_plot = 
  sim_q2_summary_df |>
  ggplot(aes(x = mu, y = avg_estimate)) +
  geom_point() +
  geom_line(aes(y = avg_estimate, color = "All samples")) +
  geom_line(aes(y = avg_estimate_rejected, color = "Rejected null only")) +
  labs(
    x = "True value of μ",
    y = "Average estimate of μ",
    title = "Average Estimates vs True μ",
    color = "Sample Type"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

estimates_plot
```

There is a clear association between effect size and power. When the true value of mu is small, the test has low power, meaning there is a smaller chance of rejecting the null hypothesis. However, as mu increases, representing a larger effect size, the power of the test also increases. Specifically, according to the the "Effect Size and Power" plot, from mu = 1 to mu = 3, the power rises sharply. Beyond mu = 4, the power approaches 1, indicating that the test is highly effective at detecting a false null hypothesis at these larger effect sizes.

The sample average of mu^hat across tests for which the null is rejected is not exactly equal to the true value of mu. 


# Problem 3. Homicide Rates Across U.S. Cities

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"

homi_data = read_csv(url)

homi_data2 = homi_data |> 
  janitor::clean_names() |> 
  mutate(
    city_state = str_c(city, ", ", state)
  )

total_vs_unsolve = homi_data2 |> 
    group_by(city) |> 
  summarize(
    Total_homicides = n(),
    Unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

knitr::kable(total_vs_unsolve, caption = "Total and Unsolved Homicides Across U.S. Cities")

```

The raw data contains `r ncol(homi_data)` variables, which are `r names(homi_data)`, and `r nrow(homi_data)` observations. For the tidy dataset, a new variable "city_state" was created.

```{r}
Bal_MD_prop = homi_data2 |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    Total_Homicides_in_Baltimore_MD = n(),
    Unsolved_Homicides_in_Baltimore_MD = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

knitr::kable(Bal_MD_prop)

Bal_MD_prop_res = prop.test(Bal_MD_prop$Unsolved_Homicides_in_Baltimore_MD, Bal_MD_prop$Total_Homicides_in_Baltimore_MD) |> 
  broom::tidy() |> 
  select(estimate,conf.low, conf.high)

knitr::kable(Bal_MD_prop_res, caption = "Proportion of Unsolved Homicides in Baltimore, MD")
```

```{r}
homicide_rate_by_cities = function(city_name) {
  
  city = homi_data2 |> 
    filter(city_state == city_name) |> 
  summarize(
    Total_Homicides = n(),
    Unsolved_Homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
  
  prop_res = prop.test(city$Unsolved_Homicides, city$Total_Homicides) |> 
    broom::tidy() |> 
    select(estimate,conf.low, conf.high)
  
  return(prop_res)
}

all_cities_states = unique(homi_data2$city_state)

homi_summary = tibble(
  city_state = all_cities_states,
  res = map(all_cities_states, homicide_rate_by_cities)
) |> 
  unnest(res)

knitr::kable(homi_summary, caption = "Proportion of Unsolved Homicides Across U.S. Cities")
```

```{r}
homi_rate_plot = 
  ggplot(homi_summary, aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  coord_flip() +
  theme_minimal()

homi_rate_plot
```

The plot displays the estimated proportion of unsolved homicides across various U.S. cities, with each point representing the mean estimate and error bars indicating confidence intervals.




